import requests
from bs4 import BeautifulSoup
import re
import os
import ssl
import socket
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from ipaddress import ip_address

class TLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        context = ssl.create_default_context()
        kwargs['ssl_context'] = context
        return super().init_poolmanager(*args, **kwargs)

def is_valid_ip(ip):
    try:
        ip_address(ip)
        return True
    except ValueError:
        return False

def try_socket_lookup(domain):
    try:
        return socket.gethostbyname(domain)
    except Exception:
        return None

urls = [
    'https://cf.vvhan.com/',
    'https://ip.164746.xyz',
    'https://github.com/hubbylei/bestcf/raw/refs/heads/main/bestcf.txt',
    'https://addressesapi.090227.xyz/CloudFlareYes'
]

ip_pattern = r'\d{1,3}(?:\.\d{1,3}){3}'

if os.path.exists('ip.txt'):
    os.remove('ip.txt')

session = requests.Session()
session.mount('https://', TLSAdapter())
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
})

ip_seen = set()
ip_list = []

for url in urls:
    print(f"ğŸ” æ­£åœ¨è¯·æ±‚ {url}")
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"[é”™è¯¯] è¯·æ±‚å¤±è´¥ {url}ï¼š{e}")
        # è¯•ç”¨ socket åŸŸåè§£æä½œä¸ºå¤‡ç”¨æ–¹å¼
        domain = re.sub(r'^https?://', '', url).split('/')[0]
        fallback_ip = try_socket_lookup(domain)
        if fallback_ip and is_valid_ip(fallback_ip):
            print(f"ğŸŒ åŸŸåè§£æè·å¾— IPï¼š{fallback_ip}")
            if fallback_ip not in ip_seen:
                ip_seen.add(fallback_ip)
                ip_list.append(fallback_ip)
        continue

    extracted = []
    content_type = response.headers.get('Content-Type', '')

    # JSON
    if 'application/json' in content_type or url.endswith('.json'):
        try:
            data = response.json()
            if isinstance(data, dict) and 'data' in data:
                for ip in data['data']:
                    if is_valid_ip(ip):
                        extracted.append(ip)
        except Exception as e:
            print(f"[é”™è¯¯] JSON è§£æå¤±è´¥ï¼š{e}")

    # çº¯æ–‡æœ¬
    elif url.endswith('.txt') or 'text/plain' in content_type:
        for line in response.text.splitlines():
            for ip in re.findall(ip_pattern, line):
                if is_valid_ip(ip):
                    extracted.append(ip)

    # HTML
    else:
        soup = BeautifulSoup(response.text, 'html.parser')
        elements = soup.find_all('tr') if 'vvhan.com' in url or '164746.xyz' in url else soup.find_all('li')
        for el in elements:
            text = ''.join(
                node for node in el.strings
            ).strip()
            for ip in re.findall(ip_pattern, text):
                if is_valid_ip(ip):
                    extracted.append(ip)

    count = 0
    for ip in extracted:
        if ip not in ip_seen:
            ip_seen.add(ip)
            ip_list.append(ip)
            count += 1
            if count == 5:
                break

print(f"âœ… å…±æå– {len(ip_list)} ä¸ªå”¯ä¸€ IPï¼ˆæ¯æºæœ€å¤šå‰ 5 æ¡ï¼Œå»é‡ï¼‰ï¼Œä¿å­˜äº ip.txtã€‚")
with open('ip.txt', 'w') as f:
    for ip in ip_list:
        f.write(ip + '\n')
